import { useState, useEffect, useRef } from "react";
import { useNavigate } from "react-router-dom";
import { NavBar, Button, Toast } from "antd-mobile";
import dogImg from "../image/dog.jpg";

export default function VoiceAnalysis() {
  const navigate = useNavigate();
  const [isRecording, setIsRecording] = useState(false);
  const [audioData, setAudioData] = useState<number[]>([]);
  const [hasRecorded, setHasRecorded] = useState(false); // æ˜¯å¦å·²å½•åˆ¶è¿‡
  const [isAnalyzing, setIsAnalyzing] = useState(false); // æ˜¯å¦æ­£åœ¨åˆ†æ
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const simulationIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const isRecordingRef = useRef<boolean>(false); // æ·»åŠ refæ¥è·Ÿè¸ªå½•åˆ¶çŠ¶æ€
  const recordingStartTime = useRef<number>(0); // å½•åˆ¶å¼€å§‹æ—¶é—´
  const audioFeaturesRef = useRef<{
    maxVolume: number;
    avgVolume: number;
    minVolume: number;
    volumeVariance: number; // éŸ³é‡æ–¹å·®
    lowFreqEnergy: number; // ä½é¢‘èƒ½é‡ (20-250Hz)
    midFreqEnergy: number; // ä¸­é¢‘èƒ½é‡ (250-4000Hz)
    highFreqEnergy: number; // é«˜é¢‘èƒ½é‡ (4000Hz+)
    dominantFrequency: number; // ä¸»å¯¼é¢‘ç‡
    frequencyStability: number; // é¢‘ç‡ç¨³å®šæ€§
    silencePeriods: number; // é™é»˜æ—¶æ®µæ•°
    volumeChanges: number[]; // éŸ³é‡å˜åŒ–åºåˆ—
    duration: number;
    sampleCount: number; // é‡‡æ ·æ•°
  }>({
    maxVolume: 0,
    avgVolume: 0,
    minVolume: 255,
    volumeVariance: 0,
    lowFreqEnergy: 0,
    midFreqEnergy: 0,
    highFreqEnergy: 0,
    dominantFrequency: 0,
    frequencyStability: 0,
    silencePeriods: 0,
    volumeChanges: [],
    duration: 0,
    sampleCount: 0,
  }); // è¯¦ç»†éŸ³é¢‘ç‰¹å¾æ•°æ®

  // åˆå§‹åŒ–é»˜è®¤å£°æ³¢æ•°æ®
  const initDefaultData = () => {
    const data = [];
    for (let i = 0; i < 50; i++) {
      data.push(Math.sin(i * 0.2) * 30 + 40); // ç”Ÿæˆæ³¢æµªå½¢çš„åˆå§‹æ•°æ®
    }
    setAudioData(data);
  };

  // ç”Ÿæˆéšæœºå£°æ³¢æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®æ—¶éŸ³é¢‘ï¼‰
  const generateRandomAudioData = () => {
    const data = [];
    for (let i = 0; i < 50; i++) {
      data.push(Math.random() * 100 + 10);
    }
    return data;
  };

  // å¼€å§‹å½•åˆ¶
  const startRecording = async () => {
    setHasRecorded(false); // é‡ç½®å½•åˆ¶çŠ¶æ€

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);

      // è®¾ç½®æ›´é«˜çš„ç²¾åº¦å’Œæ•æ„Ÿåº¦
      analyser.fftSize = 512; // å¢åŠ FFTå¤§å°è·å¾—æ›´å¥½çš„é¢‘ç‡åˆ†è¾¨ç‡
      analyser.smoothingTimeConstant = 0.3; // å‡å°‘å¹³æ»‘ä»¥è·å¾—æ›´å¿«çš„å“åº”
      analyser.minDecibels = -90; // å¢åŠ å¯¹ä½éŸ³é‡çš„æ•æ„Ÿåº¦
      analyser.maxDecibels = -10; // è®¾ç½®æœ€å¤§åˆ†è´

      source.connect(analyser);

      audioContextRef.current = audioContext;
      analyserRef.current = analyser;

      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;

      mediaRecorder.start();
      setIsRecording(true);
      isRecordingRef.current = true; // åŒæ­¥æ›´æ–°ref
      recordingStartTime.current = Date.now(); // è®°å½•å¼€å§‹æ—¶é—´

      // é‡ç½®éŸ³é¢‘ç‰¹å¾æ•°æ®
      audioFeaturesRef.current = {
        maxVolume: 0,
        avgVolume: 0,
        minVolume: 255,
        volumeVariance: 0,
        lowFreqEnergy: 0,
        midFreqEnergy: 0,
        highFreqEnergy: 0,
        dominantFrequency: 0,
        frequencyStability: 0,
        silencePeriods: 0,
        volumeChanges: [],
        duration: 0,
        sampleCount: 0,
      };

      // å¼€å§‹å®æ—¶åˆ†æéŸ³é¢‘
      analyzeAudio();

      Toast.show({ content: "ğŸ¤ çœŸå®éº¦å…‹é£å½•åˆ¶å·²å¯åŠ¨", position: "center" });
    } catch (error) {
      console.error("å½•åˆ¶å¤±è´¥:", error);
      Toast.show({
        content: "ğŸ”§ æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå£°æ³¢æ¨¡å¼",
        position: "center",
        duration: 3000,
      });
      // å¦‚æœæ— æ³•è·å–éº¦å…‹é£ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
      startSimulation();
    }
  };

  // æ¨¡æ‹Ÿå½•åˆ¶ï¼ˆå¦‚æœæ— æ³•è·å–çœŸå®éº¦å…‹é£ï¼‰
  const startSimulation = () => {
    setIsRecording(true);
    isRecordingRef.current = true; // åŒæ­¥æ›´æ–°ref

    // æ¸…é™¤ä¹‹å‰çš„å®šæ—¶å™¨
    if (simulationIntervalRef.current) {
      clearInterval(simulationIntervalRef.current);
    }

    // ç«‹å³è®¾ç½®åˆå§‹æ•°æ®
    setAudioData(generateRandomAudioData());

    // å¯åŠ¨å®šæ—¶å™¨æŒç»­æ›´æ–°æ•°æ®
    simulationIntervalRef.current = setInterval(() => {
      setAudioData(generateRandomAudioData());
    }, 150);
  };

  // å®æ—¶åˆ†æéŸ³é¢‘
  const analyzeAudio = () => {
    if (!analyserRef.current) return;

    const analyser = analyserRef.current;
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    const updateData = () => {
      // æ£€æŸ¥å½•åˆ¶çŠ¶æ€å’Œåˆ†æå™¨æ˜¯å¦è¿˜å­˜åœ¨
      if (!analyserRef.current) return;

      try {
        analyser.getByteFrequencyData(dataArray);

        // å¤„ç†éŸ³é¢‘æ•°æ® - å¢å¼ºå¯¹ä½éŸ³é‡çš„æ•æ„Ÿåº¦
        const audioDataArray = [];
        let maxValue = 0;
        let sumValue = 0;

        for (let i = 0; i < 50; i++) {
          // ä»é¢‘è°±æ•°æ®ä¸­é€‰æ‹©åˆé€‚çš„é¢‘ç‡èŒƒå›´
          const index = Math.floor(i * (bufferLength / 50));
          let value = dataArray[index];
          maxValue = Math.max(maxValue, value);
          sumValue += value;

          // å¢å¼ºæ•°æ®çš„å¯è§†åŒ–æ•ˆæœ
          value = Math.max(value * 1.5 + 20, 10); // æ”¾å¤§å¹¶æ·»åŠ åŸºç¡€é«˜åº¦
          audioDataArray.push(Math.min(value, 120)); // é™åˆ¶æœ€å¤§é«˜åº¦
        }

        // æ”¶é›†è¯¦ç»†éŸ³é¢‘ç‰¹å¾æ•°æ®ç”¨äºAIåˆ†æ
        const avgValue = sumValue / bufferLength;
        const features = audioFeaturesRef.current;

        // åŸºç¡€éŸ³é‡ç»Ÿè®¡
        features.maxVolume = Math.max(features.maxVolume, maxValue);
        features.minVolume = Math.min(features.minVolume, maxValue);
        features.avgVolume =
          (features.avgVolume * features.sampleCount + avgValue) /
          (features.sampleCount + 1);
        features.volumeChanges.push(maxValue);

        // é¢‘æ®µèƒ½é‡åˆ†æ
        const sampleRate = audioContextRef.current?.sampleRate || 44100;
        const freqBinSize = sampleRate / bufferLength;

        let lowFreqEnergy = 0,
          midFreqEnergy = 0,
          highFreqEnergy = 0;
        let maxFreqValue = 0,
          dominantFreqBin = 0;

        for (let i = 0; i < bufferLength; i++) {
          const frequency = i * freqBinSize;
          const value = dataArray[i];

          // é¢‘æ®µåˆ’åˆ†
          if (frequency < 250) {
            lowFreqEnergy += value;
          } else if (frequency < 4000) {
            midFreqEnergy += value;
          } else {
            highFreqEnergy += value;
          }

          // å¯»æ‰¾ä¸»å¯¼é¢‘ç‡
          if (value > maxFreqValue) {
            maxFreqValue = value;
            dominantFreqBin = i;
          }
        }

        // æ›´æ–°é¢‘ç‡ç‰¹å¾
        features.lowFreqEnergy = (features.lowFreqEnergy + lowFreqEnergy) / 2;
        features.midFreqEnergy = (features.midFreqEnergy + midFreqEnergy) / 2;
        features.highFreqEnergy =
          (features.highFreqEnergy + highFreqEnergy) / 2;
        features.dominantFrequency = dominantFreqBin * freqBinSize;

        // è®¡ç®—é¢‘ç‡ç¨³å®šæ€§ï¼ˆå‰åå¸§ä¸»å¯¼é¢‘ç‡å·®å¼‚ï¼‰
        const prevDominantFreq = features.dominantFrequency;
        const currentDominantFreq = dominantFreqBin * freqBinSize;
        if (features.sampleCount > 0) {
          const freqStabilityDiff = Math.abs(
            currentDominantFreq - prevDominantFreq
          );
          features.frequencyStability =
            (features.frequencyStability + freqStabilityDiff) / 2;
        }

        // æ£€æµ‹é™é»˜æ—¶æ®µï¼ˆéŸ³é‡ä½äºé˜ˆå€¼ï¼‰
        if (maxValue < 30) {
          features.silencePeriods++;
        }

        features.sampleCount++;

        // è°ƒè¯•ä¿¡æ¯ï¼šæ¯ç§’è¾“å‡ºä¸€æ¬¡æœ€å¤§éŸ³é‡å€¼
        if (Math.random() < 0.02) {
          // å¤§çº¦æ¯ç§’è¾“å‡ºä¸€æ¬¡ (2% æ¦‚ç‡ * ~60fps)
          console.log(
            "ğŸµ å®æ—¶éŸ³é¢‘åˆ†æ - æœ€å¤§éŸ³é‡å€¼:",
            maxValue,
            "å¹³å‡éŸ³é‡:",
            avgValue.toFixed(2),
            "åŸå§‹æ•°æ®èŒƒå›´:",
            Math.min(...dataArray),
            "-",
            Math.max(...dataArray)
          );
        }

        setAudioData(audioDataArray);

        // åªæœ‰åœ¨å½•åˆ¶çŠ¶æ€ä¸‹æ‰ç»§ç»­æ›´æ–°
        if (isRecordingRef.current) {
          animationFrameRef.current = requestAnimationFrame(updateData);
        }
      } catch (error) {
        console.error("éŸ³é¢‘åˆ†æé”™è¯¯:", error);
      }
    };

    updateData();
  };

  // åœæ­¢å½•åˆ¶
  const stopRecording = () => {
    setIsRecording(false);
    isRecordingRef.current = false; // åŒæ­¥æ›´æ–°ref
    setHasRecorded(true); // æ ‡è®°å·²å½•åˆ¶è¿‡

    // è®¡ç®—å½•åˆ¶æ—¶é•¿
    const duration = (Date.now() - recordingStartTime.current) / 1000;
    audioFeaturesRef.current.duration = duration;

    // è®¡ç®—æœ€ç»ˆçš„éŸ³é¢‘ç‰¹å¾ç»Ÿè®¡
    const features = audioFeaturesRef.current;
    const volumeVariance =
      features.volumeChanges.length > 1
        ? features.volumeChanges.reduce(
            (sum, v) => sum + Math.pow(v - features.avgVolume, 2),
            0
          ) / features.volumeChanges.length
        : 0;
    features.volumeVariance = volumeVariance;

    console.log("å½•åˆ¶å®Œæˆ - è¯¦ç»†éŸ³é¢‘ç‰¹å¾æ•°æ®:", {
      duration: duration.toFixed(2) + "ç§’",
      sampleCount: features.sampleCount,
      volume: {
        max: features.maxVolume.toFixed(1),
        avg: features.avgVolume.toFixed(1),
        min: features.minVolume.toFixed(1),
        variance: volumeVariance.toFixed(1),
      },
      frequency: {
        dominant: features.dominantFrequency.toFixed(1) + "Hz",
        stability: features.frequencyStability.toFixed(1),
        lowEnergy: features.lowFreqEnergy.toFixed(1),
        midEnergy: features.midFreqEnergy.toFixed(1),
        highEnergy: features.highFreqEnergy.toFixed(1),
      },
      behavior: {
        silencePeriods: features.silencePeriods,
        volumeChanges: features.volumeChanges.slice(-5), // æœ€å5ä¸ªéŸ³é‡å€¼
      },
    });

    // ä¸è‡ªåŠ¨æ˜¾ç¤ºåˆ†æç»“æœ

    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current.stream
        .getTracks()
        .forEach((track) => track.stop());
    }

    if (audioContextRef.current) {
      audioContextRef.current.close();
    }

    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }

    // æ¸…é™¤æ¨¡æ‹Ÿå®šæ—¶å™¨
    if (simulationIntervalRef.current) {
      clearInterval(simulationIntervalRef.current);
    }

    Toast.show({ content: "å£°éŸ³æ”¶é›†å®Œæˆ", position: "center" });
  };

  // åˆ†æéŸ³é‡å˜åŒ–æ¨¡å¼
  const analyzeVolumePattern = (volumeChanges: number[]) => {
    if (volumeChanges.length < 5) return "æ•°æ®ä¸è¶³";

    const recentChanges = volumeChanges.slice(-20); // æœ€è¿‘20ä¸ªé‡‡æ ·ç‚¹
    let increasingCount = 0;
    let decreasingCount = 0;
    let stableCount = 0;

    for (let i = 1; i < recentChanges.length; i++) {
      const diff = recentChanges[i] - recentChanges[i - 1];
      if (diff > 5) increasingCount++;
      else if (diff < -5) decreasingCount++;
      else stableCount++;
    }

    const total = increasingCount + decreasingCount + stableCount;
    if (increasingCount / total > 0.4) return "é€’å¢å‹(å…´å¥‹/æ¿€åŠ¨)";
    if (decreasingCount / total > 0.4) return "é€’å‡å‹(ç–²æƒ«/å¹³æ¯)";
    if (stableCount / total > 0.6) return "ç¨³å®šå‹(å¹³é™)";
    return "æ³¢åŠ¨å‹(ä¸å®‰/ç„¦è™‘)";
  };

  // è¿›è¡Œå£°éŸ³åˆ†æ
  const analyzeRecordedAudio = async () => {
    setIsAnalyzing(true);
    Toast.show({ content: "ğŸµ æ­£åœ¨åˆ†æå£°éŸ³æ•°æ®...", position: "center" });

    try {
      // å‡†å¤‡å‘é€åˆ°åç«¯çš„è¯¦ç»†éŸ³é¢‘ç‰¹å¾æ•°æ®
      const features = audioFeaturesRef.current;

      // è®¡ç®—éŸ³é¢‘ç‰¹å¾æŒ‡æ ‡
      const volumeRange = features.maxVolume - features.minVolume;
      const totalEnergy =
        features.lowFreqEnergy +
        features.midFreqEnergy +
        features.highFreqEnergy;
      const frequencyDistribution = {
        low:
          totalEnergy > 0
            ? ((features.lowFreqEnergy / totalEnergy) * 100).toFixed(1)
            : "0",
        mid:
          totalEnergy > 0
            ? ((features.midFreqEnergy / totalEnergy) * 100).toFixed(1)
            : "0",
        high:
          totalEnergy > 0
            ? ((features.highFreqEnergy / totalEnergy) * 100).toFixed(1)
            : "0",
      };

      const analysisData = {
        // åŸºç¡€ä¿¡æ¯
        duration: features.duration.toFixed(1),
        sampleCount: features.sampleCount,

        // éŸ³é‡ç‰¹å¾
        volumeStats: {
          max: features.maxVolume.toFixed(1),
          avg: features.avgVolume.toFixed(1),
          min: features.minVolume.toFixed(1),
          variance: features.volumeVariance.toFixed(1),
          range: volumeRange.toFixed(1),
        },

        // é¢‘ç‡ç‰¹å¾
        frequencyStats: {
          dominantFreq: features.dominantFrequency.toFixed(1),
          stability: features.frequencyStability.toFixed(1),
          distribution: frequencyDistribution,
        },

        // è¡Œä¸ºç‰¹å¾
        behaviorStats: {
          silenceRatio:
            features.sampleCount > 0
              ? (
                  (features.silencePeriods / features.sampleCount) *
                  100
                ).toFixed(1)
              : "0",
          silencePeriods: features.silencePeriods,
          volumePattern: analyzeVolumePattern(features.volumeChanges),
        },
      };

      console.log("å‘é€åˆ°AIåˆ†æçš„æ•°æ®:", analysisData);

      // è°ƒç”¨åç«¯AIåˆ†ææ¥å£
      const token = localStorage.getItem("token");
      const response = await fetch("/api/ai/voice-analysis", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${token}`,
        },
        body: JSON.stringify(analysisData),
      });

      if (!response.ok) {
        throw new Error(`HTTPé”™è¯¯! çŠ¶æ€: ${response.status}`);
      }

      const result = await response.json();
      console.log("AIåˆ†æç»“æœ:", result);

      if (result.success) {
        // å°†AIåˆ†æç»“æœä¿å­˜åˆ°localStorageï¼Œä¼ é€’ç»™ç»“æœé¡µé¢
        localStorage.setItem(
          "voiceAnalysisResult",
          JSON.stringify(result.data)
        );

        Toast.show({
          content: "ğŸ‰ åˆ†æå®Œæˆï¼",
          position: "center",
        });

        // è·³è½¬åˆ°ç»“æœé¡µé¢
        navigate("/voice-analysis-result");
      } else {
        throw new Error(result.message || "åˆ†æå¤±è´¥");
      }
    } catch (error) {
      console.error("å£°éŸ³åˆ†æå¤±è´¥:", error);
      Toast.show({
        content: "âŒ åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç»“æœ",
        position: "center",
        duration: 3000,
      });

      // åˆ†æå¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤ç»“æœ
      const defaultResult = {
        emotions: [
          {
            emotion: "å¹³é™",
            percentage: 30,
            color: "#27AE60",
            description: "å£°éŸ³å¹³ç¨³ï¼ŒèŠ‚å¥è§„å¾‹",
          },
          {
            emotion: "ç„¦è™‘",
            percentage: 25,
            color: "#F39C12",
            description: "å£°éŸ³èŠ‚å¥è¾ƒå¿«ï¼ŒéŸ³è°ƒåé«˜",
          },
          {
            emotion: "æ‚²ä¼¤",
            percentage: 20,
            color: "#4A90E2",
            description: "æ£€æµ‹åˆ°ä½æ²‰ã€ç¼“æ…¢çš„å£°éŸ³ç‰¹å¾",
          },
          {
            emotion: "ä¸å®‰",
            percentage: 15,
            color: "#9B59B6",
            description: "å£°éŸ³é¢‘ç‡ç•¥æœ‰æ³¢åŠ¨",
          },
          {
            emotion: "æƒŠæ€’",
            percentage: 10,
            color: "#E74C3C",
            description: "çŸ­æš‚çš„é«˜é¢‘å£°éŸ³çˆ†å‘",
          },
        ],
        summary:
          "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•è¿æ¥AIåˆ†ææœåŠ¡ã€‚æ˜¾ç¤ºé»˜è®¤åˆ†æç»“æœï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œåé‡è¯•ã€‚",
        recommendations: [
          "æ£€æŸ¥ç½‘ç»œè¿æ¥çŠ¶æ€",
          "é‡æ–°å½•åˆ¶å® ç‰©å£°éŸ³è¿›è¡Œåˆ†æ",
          "è§‚å¯Ÿå® ç‰©çš„æ—¥å¸¸è¡Œä¸ºæ¨¡å¼",
          "å¦‚æœ‰å¼‚å¸¸ï¼Œå»ºè®®å’¨è¯¢ä¸“ä¸šå…½åŒ»",
        ],
      };

      localStorage.setItem(
        "voiceAnalysisResult",
        JSON.stringify(defaultResult)
      );
      navigate("/voice-analysis-result");
    } finally {
      setIsAnalyzing(false);
    }
  };

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†èµ„æº
  useEffect(() => {
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (simulationIntervalRef.current) {
        clearInterval(simulationIntervalRef.current);
      }
    };
  }, []);

  // é¡µé¢åŠ è½½æ—¶åˆå§‹åŒ–
  useEffect(() => {
    // é¦–å…ˆæ˜¾ç¤ºé»˜è®¤æ•°æ®
    initDefaultData();

    // ç„¶åå°è¯•å¼€å§‹å½•åˆ¶
    const initRecording = async () => {
      await startRecording();
    };

    // å»¶è¿Ÿä¸€ç‚¹æ—¶é—´å¼€å§‹å½•åˆ¶ï¼Œè®©ç”¨æˆ·å…ˆçœ‹åˆ°ç•Œé¢
    const timer = setTimeout(initRecording, 500);

    return () => clearTimeout(timer);
  }, []);

  // è·å–æŸ±å­é¢œè‰²
  const getBarColor = (value: number, index: number) => {
    const colors = [
      "#3498db", // è“è‰²
      "#f1c40f", // é»„è‰²
      "#e74c3c", // çº¢è‰²
      "#2ecc71", // ç»¿è‰²
      "#9b59b6", // ç´«è‰²
      "#e67e22", // æ©™è‰²
      "#1abc9c", // é’ç»¿è‰²
      "#34495e", // æ·±ç°è‰²
    ];
    // æ ¹æ®éŸ³é‡å€¼è°ƒæ•´é¢œè‰²é€æ˜åº¦
    const opacity = Math.min(value / 100, 1);
    const baseColor = colors[index % colors.length];
    return `${baseColor}${Math.floor(opacity * 255)
      .toString(16)
      .padStart(2, "0")}`;
  };

  return (
    <div
      style={{
        minHeight: "100vh",
        background: `linear-gradient(rgba(255,255,255,0.9), rgba(255,255,255,0.7)), url(${dogImg})`,
        backgroundSize: "cover",
        backgroundPosition: "center",
      }}
    >
      {/* é¡¶éƒ¨å¯¼èˆªæ  */}
      <NavBar
        back="è¿”å›"
        onBack={() => {
          stopRecording();
          navigate(-1);
        }}
        style={{
          backgroundColor: "rgba(255,255,255,0.95)",
          color: "#333",
          backdropFilter: "blur(10px)",
        }}
      >
        å£°æ³¢å›¾
      </NavBar>

      {/* é¡µé¢å†…å®¹ */}
      <div style={{ padding: "20px", textAlign: "center" }}>
        {/* å£°æ³¢å›¾æ ‡é¢˜ */}
        <div
          style={{
            fontSize: "24px",
            fontWeight: "bold",
            color: "#ff8c00",
            marginBottom: "30px",
            textShadow: "0 2px 4px rgba(0,0,0,0.1)",
          }}
        >
          å£°æ³¢å›¾
        </div>

        {/* å£°æ³¢å¯è§†åŒ–åŒºåŸŸ */}
        <div
          style={{
            backgroundColor: "rgba(255,255,255,0.95)",
            borderRadius: "12px",
            padding: "30px 20px",
            marginBottom: "20px",
            boxShadow: "0 4px 20px rgba(0,0,0,0.1)",
            backdropFilter: "blur(10px)",
          }}
        >
          {/* å£°æ³¢å›¾ */}
          <div
            style={{
              display: "flex",
              alignItems: "end",
              justifyContent: "center",
              height: "150px",
              gap: "3px",
              marginBottom: "20px",
            }}
          >
            {audioData.map((value, index) => (
              <div
                key={index}
                style={{
                  width: "6px",
                  height: `${Math.max(value * 1.2, 10)}px`,
                  backgroundColor: getBarColor(value, index),
                  borderRadius: "3px 3px 0 0",
                  transition: "height 0.1s ease",
                  opacity: isRecording ? 1 : 0.5,
                }}
              />
            ))}
          </div>

          {/* çŠ¶æ€æ–‡å­— */}
          <div
            style={{
              fontSize: "16px",
              color: "#666",
              marginTop: "10px",
            }}
          >
            {isRecording
              ? "æ­£åœ¨æ”¶é›†..."
              : hasRecorded
              ? "æ”¶é›†å®Œæˆ"
              : "å‡†å¤‡æ”¶é›†"}
          </div>
        </div>

        {/* æ§åˆ¶æŒ‰é’®åŒºåŸŸ */}
        <div
          style={{
            position: "fixed",
            bottom: "120px",
            left: "50%",
            transform: "translateX(-50%)",
            zIndex: 10,
          }}
        >
          <Button
            onClick={
              isRecording
                ? stopRecording
                : hasRecorded
                ? analyzeRecordedAudio
                : startRecording
            }
            disabled={isAnalyzing}
            style={{
              width: "200px",
              height: "50px",
              background: isAnalyzing
                ? "linear-gradient(135deg, #999 0%, #777 100%)"
                : "linear-gradient(135deg, #d4994b 0%, #c8955a 100%)",
              border: "none",
              borderRadius: "25px",
              fontSize: "16px",
              fontWeight: "bold",
              color: "white",
              boxShadow: "0 4px 12px rgba(212, 153, 75, 0.3)",
              opacity: isAnalyzing ? 0.7 : 1,
            }}
          >
            {isAnalyzing
              ? "AIåˆ†æä¸­..."
              : isRecording
              ? "åœæ­¢å£°éŸ³æ”¶é›†"
              : hasRecorded
              ? "è¿›è¡Œå£°éŸ³åˆ†æ"
              : "å¼€å§‹å£°éŸ³æ”¶é›†"}
          </Button>
        </div>

        {/* åº•éƒ¨æç¤ºæ–‡å­— */}
        <div
          style={{
            position: "fixed",
            bottom: "60px",
            left: "50%",
            transform: "translateX(-50%)",
            fontSize: "14px",
            color: "white",
            textShadow: "0 2px 4px rgba(0,0,0,0.5)",
            textAlign: "center",
          }}
        >
          {isRecording
            ? "å°†éº¦å…‹é£å¯¹å‡†å® ç‰© æ”¶é›†å® ç‰©å£°éŸ³"
            : hasRecorded
            ? "ç‚¹å‡»æŒ‰é’®å¼€å§‹åˆ†ææ”¶é›†çš„å£°éŸ³"
            : "å°†éº¦å…‹é£å¯¹å‡†å® ç‰© æ”¶é›†å® ç‰©å£°éŸ³"}
        </div>
      </div>
    </div>
  );
}
